{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 67158,
          "databundleVersionId": 7460879,
          "sourceType": "competition"
        },
        {
          "sourceId": 2220588,
          "sourceType": "datasetVersion",
          "datasetId": 1333551
        },
        {
          "sourceId": 7545804,
          "sourceType": "datasetVersion",
          "datasetId": 4394459
        }
      ],
      "dockerImageVersionId": 30097,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'image-search:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F67158%2F7460879%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240203%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240203T150219Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dab6374445f72c957fe4604d38d1b8fe5ac7800f977609a4d4cbd41601dc1bbc4f23a00bdefc06f7f46152643bcda05de9b6e125755f786461b528c4e1a19bc8a0383f53894f546833a10d57c1c9023460074c4e8058a2f5b2dff6f8386564ac78b535eeba84d4cb4e97961ca7bda759374df9ada48794745b5e7fc0fe56d8989c00402b23c229054f921d5b85c2d14baf4be16aa3316f9c144f0af9fe65466eba37c5e781968df218f33090fa5d9484c89bf6c594b2956bda9ba8c9f0609533423a6e30746f258937101586ef2d4a04289e7d800bc14f98c61c95588140357252622ce3be0c621e268db6f840711676a95e5044ead215e3f3e01e53744c8e607,arttrain:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4394459%2F7545804%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240203%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240203T150219Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5ca451a133a3e54490ae982d8c2d1303dd3b277c7b7cf1b61d091d8cfff0d852411890a26a29dc2741939af624be3a5e9f46102bf7998673544f7d153c9b09d1671080897fa691a58536c295bacee8aa194d4cdf980095c02054b1b9ba4085519ba550eb84c9d4e03f14650f5959545f32299e70345d1524c9addb59281d10ee1e24efb169ac7477132c5027f8e1480ac0b80d7e9497fe144503ade8bb72a41ba29aea8555bd652d33203b1a243aac87221320a0532c9919321b72dfce48b8ecb00f4487b4be121f8b8d7a7e5ee375a85fec416832c5923b9f1aec752c55d37ee81d21ac99fe6311fc438b30bd2a2df4d4f510e1f70925bfcfe6e73f3aea34c1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Q7i40uMafgZM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing CLIP"
      ],
      "metadata": {
        "id": "pBe0qVoifgZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-03T14:56:04.793958Z",
          "iopub.execute_input": "2024-02-03T14:56:04.794277Z",
          "iopub.status.idle": "2024-02-03T14:56:23.439506Z",
          "shell.execute_reply.started": "2024-02-03T14:56:04.794205Z",
          "shell.execute_reply": "2024-02-03T14:56:23.438436Z"
        },
        "trusted": true,
        "id": "HBFC5u6rfgZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, CLIPModel,CLIPProcessor,AutoModel\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from tqdm.notebook import tqdm\n",
        "#from .autonotebook import tqdm as notebook_tqdm\n",
        "import clip\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "BATCH_SIZE = 8\n",
        "EPOCH = 5\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-03T14:46:08.558151Z",
          "iopub.execute_input": "2024-02-03T14:46:08.558545Z",
          "iopub.status.idle": "2024-02-03T14:46:08.58123Z",
          "shell.execute_reply.started": "2024-02-03T14:46:08.55851Z",
          "shell.execute_reply": "2024-02-03T14:46:08.579565Z"
        },
        "trusted": true,
        "id": "Ya6VW3DsfgZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Model and Data"
      ],
      "metadata": {
        "id": "KNZxPA7afgZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = '/kaggle/input/image-search/test/images'\n",
        "query_dir = '/kaggle/input/image-search/queries/queries'\n",
        "submission = pd.read_csv('/kaggle/input/image-search/sample_submission.csv')\n",
        "train_dir = '/kaggle/input/arttrain/train'\n",
        "#model = CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K').cuda\n",
        "#processor = AutoProcessor.from_pretrained('laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K')\n",
        "model = AutoModel.from_pretrained('laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K').cuda().eval()\n",
        "processor = CLIPProcessor.from_pretrained('laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K')\n",
        "submission['dot_class'] = 22\n",
        "submission['cosine_class'] = 22"
      ],
      "metadata": {
        "trusted": true,
        "id": "LVHQVorPfgZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.TrivialAugmentWide(),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "v0PlZ48xgRZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all name folders (assuming \"name\" is the desired naming convention)\n",
        "name_folders = [name for name in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, name))]\n",
        "img = []\n",
        "name = []\n",
        "for var in name_folders:\n",
        "  img2 = glob.glob(train_dir+'/'+var+\"/*.jpg\")\n",
        "  img += img2\n",
        "  name+= [var]*len(img2)\n",
        "name = [x for x in name if x!='.ipynb_checkpoints']"
      ],
      "metadata": {
        "id": "mXInmxokfgZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = name\n",
        "df['path'] = img\n",
        "df"
      ],
      "metadata": {
        "id": "9ySE4qJNfgZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {}\n",
        "for i in set(df['id'].tolist()):\n",
        "    d[i] = df[df['id'] == i]['path'].tolist()\n",
        "sel_img = df['path'].tolist()\n",
        "sel_id = list(set(df['id'].tolist()))"
      ],
      "metadata": {
        "id": "np0no_wkfgZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.Tensor(processor(images=[Image.open(sel_img[0])]).pixel_values[0]).shape"
      ],
      "metadata": {
        "id": "-0d6E2PMfgZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths, test_img_paths = train_test_split(sel_id, test_size=0.2, random_state=42)\n",
        "d_train = {k: d[k] for k in train_img_paths}\n",
        "d_test = {k: d[k] for k in test_img_paths}\n",
        "len(d_train), len(d_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lWOVl04OfgZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.ToPILImage()\n",
        "class LogoDataset(Dataset):\n",
        "    def __init__(self, data, preprocess):\n",
        "        self.preprocess = preprocess\n",
        "        self.img_paths = []\n",
        "        self.captions = []\n",
        "        for cap, img_path in data.items():\n",
        "            for img in img_path:\n",
        "                self.img_paths.append(img)\n",
        "                self.captions.append(cap)\n",
        "                self.img_paths.append(img+'tmp')\n",
        "                self.captions.append(cap)\n",
        "        self.processed_cache = {}\n",
        "        for _ ,img_path in data.items():\n",
        "            for i in range(len(img_path)):\n",
        "                img = Image.open(sel_img[0])\n",
        "                imgc = transform(train_transform(img))\n",
        "                self.processed_cache[img_path[i]] = torch.Tensor(processor(images=[Image.open(sel_img[0])]).pixel_values[0])\n",
        "                self.processed_cache[img_path[i]+'tmp'] = torch.Tensor(processor(images=[imgc]).pixel_values[0])\n",
        "        # print(self.img_paths)\n",
        "        self.path2label = {path: self.img_paths.index(path) for path in self.img_paths}\n",
        "\n",
        "    def check(self):\n",
        "        print(self.captions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = self.processed_cache[img_path]\n",
        "        caption = self.captions[idx]\n",
        "        label = self.path2label[img_path]\n",
        "        return image, caption, label"
      ],
      "metadata": {
        "trusted": true,
        "id": "3D2eiVsVfgZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = LogoDataset(d_train, processor)\n",
        "test_dataset = LogoDataset(d_test, processor)\n",
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZMTVENcLfgZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "eDRph3n8fgZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/pytorch/blob/e5742494f6080c8e6f43c37689fc18a7c4b39dfd/torch/utils/data/dataloader.py#L145\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = labels\n",
        "        self.labels_set = list(set(self.labels.numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size\n",
        "\n",
        "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
        "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
        "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
        "\n",
        "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
        "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dXd9ftZ5fgZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(train_sampler):\n",
        "    labels = []\n",
        "    for idx in item:\n",
        "        label = train_dataset[idx][2]\n",
        "        labels.append(label)\n",
        "    break\n",
        "len(labels), len(set(labels))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6qzWJT4zfgZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    imgs, txts, labels = batch\n",
        "    print(imgs.shape)\n",
        "    print(len(txts))\n",
        "    print(labels)\n",
        "    print(labels.shape)\n",
        "    print(torch.unique(labels).shape)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "id": "lUjTxWkIfgZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train (Fine-Tuning)"
      ],
      "metadata": {
        "id": "ID_cealafgZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for p in model.parameters():\n",
        "    if count==0:\n",
        "        print(p)\n",
        "        break"
      ],
      "metadata": {
        "id": "ElhtW48VfgZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model.float()\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"
      ],
      "metadata": {
        "trusted": true,
        "id": "YCoQ0NVdfgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "VtsyFxnnjzh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor(text =['this is earth','I screaming'], padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "3mK5N9wifgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_text_features( **processor(text =['this is earth','I screaming'], padding=True, return_tensors=\"pt\").to('cuda'))"
      ],
      "metadata": {
        "id": "sRAsBjgUfgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor(images=[Image.open(sel_img[0])], padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "UFCi9kW_fgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_te_loss = 1e5\n",
        "best_ep = -1\n",
        "for epoch in range(EPOCH):\n",
        "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
        "    step = 0\n",
        "    tr_loss = 0\n",
        "    model.train()\n",
        "    pbar = tqdm(train_dataloader, leave=False)\n",
        "    print(len(pbar))\n",
        "    for batch in pbar:\n",
        "        step += 1\n",
        "        if step == 1:\n",
        "            print('start')\n",
        "        if step%10 == 0:\n",
        "            print(step)\n",
        "        optimizer.zero_grad()\n",
        "        images, texts, _ = batch\n",
        "        images = images.cuda()\n",
        "        texts = processor(text = texts, padding=True, return_tensors=\"pt\").to('cuda')\n",
        "        logits_per_image = model.get_image_features(images)\n",
        "        logits_per_text = model.get_text_features(**texts)\n",
        "        #logits_per_image, logits_per_text = model(*images, **texts)\n",
        "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "\n",
        "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "        total_loss.backward()\n",
        "        tr_loss += total_loss.item()\n",
        "        if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            #convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            #clip.model.convert_weights(model)\n",
        "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
        "    tr_loss /= step\n",
        "\n",
        "    step = 0\n",
        "    te_loss = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_pbar = tqdm(test_dataloader, leave=False)\n",
        "        for batch in test_pbar:\n",
        "            step += 1\n",
        "            images, texts, _ = batch\n",
        "            images = images.to(device)\n",
        "            texts = processor(text = texts, padding=True, return_tensors=\"pt\").to('cuda')\n",
        "            logits_per_image = model.get_image_features(images)\n",
        "            logits_per_text = model.get_text_features(**texts)\n",
        "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "\n",
        "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "            te_loss += total_loss.item()\n",
        "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
        "        te_loss /= step\n",
        "\n",
        "    if te_loss < best_te_loss:\n",
        "        best_te_loss = te_loss\n",
        "        best_ep = epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
        "torch.save(model.state_dict(), \"last_model.pt\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "cvB72n43fgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search"
      ],
      "metadata": {
        "id": "g-4vlPn-fgZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"../input/clipfinetuneweights/best_model.pt\"))\n",
        "NUM_NEG = 127\n",
        "NUM_TEST = 1000"
      ],
      "metadata": {
        "trusted": true,
        "id": "YVYKlVHDfgZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit\n",
        "with torch.no_grad():\n",
        "    query_images = []\n",
        "    query_classes = []\n",
        "    for file in os.listdir(query_dir):\n",
        "        inputs = processor(images=[Image.open(os.path.join(query_dir, file)).convert('L')], return_tensors='pt').to('cuda')\n",
        "        outputs = model.get_image_features(inputs.pixel_values).cuda()\n",
        "        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
        "        query_images.append(outputs)\n",
        "        query_classes.append(int(file[:-4]))\n",
        "    query_images = torch.cat(query_images)\n",
        "    for idx, row in submission.iterrows():\n",
        "        if not pd.isna(row['class']):\n",
        "            continue\n",
        "        inputs = processor(images=[Image.open(os.path.join(src_dir, row['img_file'])).convert('L')], return_tensors='pt').to('cuda')\n",
        "        outputs = model.get_image_features(inputs.pixel_values).cuda()\n",
        "        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
        "        values = outputs @ query_images.T\n",
        "        if values.softmax(1).max() > .055:\n",
        "            submission.at[idx, 'dot_class'] = query_classes[values.argmax().tolist()]\n",
        "        cosine = torch.cosine_similarity(outputs, query_images)\n",
        "        if cosine.max() > 0.6:\n",
        "            submission.at[idx, 'cosine_class'] = query_classes[cosine.argmax().tolist()]\n",
        "    sub = submission[['img_file',]]\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "lBgHQboifgZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub['class'] = submission['dot_class']\n",
        "sub.to_csv('dot_product_0.055_ft-3_try.csv', index=False)\n",
        "sub['class'] = submission['cosine_class']\n",
        "sub.to_csv('cosine_similarity_0.6_ft-3_try.csv', index=False)"
      ],
      "metadata": {
        "id": "4xGLCDxUfgZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckXHSXObfgZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}